# hbase-packet-inspector

hbase-packet-inspector analyzes packets to/from HBase region server and builds
an in-memory database. You can query the database in the command-line shell or
via its web SQL interface.

## Usage

hbase-packet-inspector can read tcpdump output files or a live capture from a
network interface (the latter requires root permission).

```
Usage:
  hbase-packet-inspector [OPTIONS] [-i INTERFACE]
  hbase-packet-inspector [OPTIONS] FILES...

Options:
  -h --help                 Show this message
  -i --interface=INTERFACE  Network interface to monitor
  -p --port=PORT            Port to monitor (default: 16020 and 60020)
  -c --count=COUNT          Maximum number of packets to process
  -d --duration=DURATION    Number of seconds to capture packets
  -k --kafka=SERVERS/TOPIC  Kafka bootstrap servers and the name of the topic
                              TOPIC:
                                T:     Both requests and responses to T
                                T1/T2: Requests to T1, responses to T2
                                T/:    Requests to T, responses are ignored
                                /T:    Requests are ignored, responses to T
  -v --verbose              Verbose output
```

If `--kafka` option is used, hbase-packet-inspector will send records for
`requests` and `responses` table to the specified Kafka cluster as json
records, instead of creating in-memory database.

You can send additional key-value pairs to Kafka as follows:

```sh
--kafka "bootstrap1:9092,bootstrap2:9092/hbase-packets?service=twitter&cluster=feed"
```

## Example

```sh
# Reading from tcpdump output
tcpdump -s 0 -c 100000 -nn -w dump.pcap port 16020 or port 60020
./hbase-packet-inspector dump.pcap

# Use readline wrapper
rlwrap ./hbase-packet-inspector dump.pcap

# Reading from a live capture; captures the packets until you press enter
sudo ./hbase-packet-inspector
```

Alternatively, you can start it with java command to pass extra JVM options.

```sh
java -Xmx2g -jar hbase-packet-inspector --help
```

## Schema

- requests
    - actions (for multi requests)
- responses
    - results (for multi responses)

Note that `call_id` is not globally unique nor monotonically increasing. Join
between the tables should be performed on (`client`, `port`, `call_id`)
columns.

Currently, only records for `responses` table can be sent to Kafka.

## Build

```sh
# Requires leiningen
lein bin
```

## Test

```sh
lein test

# For coverage report, use lein-cloverage (https://github.com/cloverage/cloverage)
lein cloverage
```

### Generating fixtures

Some of the test cases read actual tcpdump output files for a predefined
series of HBase client operations. They were generated by running
[generate-fixtures.sh](/dev-resources/generate-fixtures.sh) inside Docker
container built with the included [Dockerfile](Dockerfile).

```sh
docker build -t hpi-test-env .
docker run -v $(PWD)/dev-resources:/data -it hpi-test-env /data/generate-fixtures.sh
```

## License

This software is licensed under the [Apache 2 license](LICENSE.txt), quoted below.

Copyright 2017 Kakao Corp. <http://www.kakaocorp.com>

Licensed under the Apache License, Version 2.0 (the "License"); you may not
use this project except in compliance with the License. You may obtain a copy
of the License at http://www.apache.org/licenses/LICENSE-2.0.

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
License for the specific language governing permissions and limitations under
the License.
